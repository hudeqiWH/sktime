{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b5d722c",
   "metadata": {},
   "source": [
    "# How to Load and Format Data for sktime\n",
    "\n",
    "This guide shows you how to load data from various sources and format it correctly for use with sktime.\n",
    "\n",
    "## Overview\n",
    "\n",
    "This guide covers:\n",
    "1. Loading data from different file formats\n",
    "2. Converting to sktime-compatible formats\n",
    "3. Handling different time series structures\n",
    "4. Common data formatting issues and solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77044a56",
   "metadata": {},
   "source": [
    "## 1. Loading Data from CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1884352",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sktime.datatypes import check_raise, mtype, scitype\n",
    "\n",
    "# Create sample CSV data for demonstration\n",
    "print(\"Creating sample data files...\")\n",
    "\n",
    "# 1. Simple time series CSV\n",
    "dates = pd.date_range(\"2020-01-01\", periods=100, freq=\"D\")\n",
    "values = np.random.randn(100).cumsum() + 100\n",
    "simple_df = pd.DataFrame({\"date\": dates, \"sales\": values, \"region\": \"North\"})\n",
    "\n",
    "print(\"Sample CSV structure:\")\n",
    "print(simple_df.head())\n",
    "print(f\"Shape: {simple_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ef9cf3",
   "metadata": {},
   "source": [
    "### Loading and Converting CSV Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad93f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Direct conversion to time series\n",
    "print(\"Method 1: Direct conversion\")\n",
    "y_simple = simple_df.set_index(\"date\")[\"sales\"]\n",
    "print(f\"Result type: {type(y_simple)}\")\n",
    "print(f\"Scitype: {scitype(y_simple)}\")\n",
    "print(f\"Mtype: {mtype(y_simple)}\")\n",
    "print(f\"Index type: {type(y_simple.index)}\")\n",
    "print(f\"Sample: {y_simple.head()}\")\n",
    "\n",
    "# Method 2: Parse dates during loading\n",
    "print(\"\\nMethod 2: Parse dates during loading\")\n",
    "# Simulate reading from CSV with date parsing\n",
    "y_parsed = pd.Series(\n",
    "    simple_df[\"sales\"].values, index=pd.to_datetime(simple_df[\"date\"]), name=\"sales\"\n",
    ")\n",
    "print(f\"Result: {y_parsed.head()}\")\n",
    "\n",
    "# Method 3: Handle different date formats\n",
    "print(\"\\nMethod 3: Different date formats\")\n",
    "# Create data with string dates\n",
    "string_dates = [\"2020-01-01\", \"2020-01-02\", \"2020-01-03\", \"2020-01-04\", \"2020-01-05\"]\n",
    "string_values = [100, 101, 99, 102, 98]\n",
    "\n",
    "y_string_dates = pd.Series(\n",
    "    string_values, index=pd.to_datetime(string_dates), name=\"values\"\n",
    ")\n",
    "print(f\"String dates converted: {y_string_dates}\")\n",
    "\n",
    "# Validate the data\n",
    "try:\n",
    "    check_raise(y_simple, mtype=\"pd.Series\", scitype=\"Series\")\n",
    "    print(\"\\n✓ Data validation successful!\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n✗ Data validation failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628b89af",
   "metadata": {},
   "source": [
    "## 2. Loading Panel Data (Multiple Time Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d41657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create panel data sample\n",
    "print(\"Creating panel data...\")\n",
    "\n",
    "panel_data = []\n",
    "regions = [\"North\", \"South\", \"East\", \"West\"]\n",
    "dates = pd.date_range(\"2020-01-01\", periods=50, freq=\"D\")\n",
    "\n",
    "for region in regions:\n",
    "    base_value = np.random.randint(80, 120)\n",
    "    trend = np.random.uniform(-0.1, 0.1)\n",
    "    noise = np.random.randn(50) * 5\n",
    "\n",
    "    values = base_value + np.arange(50) * trend + noise\n",
    "\n",
    "    for i, (date, value) in enumerate(zip(dates, values)):\n",
    "        panel_data.append(\n",
    "            {\n",
    "                \"region\": region,\n",
    "                \"date\": date,\n",
    "                \"sales\": value,\n",
    "                \"day_of_week\": date.dayofweek,\n",
    "            }\n",
    "        )\n",
    "\n",
    "panel_df = pd.DataFrame(panel_data)\n",
    "print(f\"Panel data shape: {panel_df.shape}\")\n",
    "print(f\"Sample:\\n{panel_df.head(10)}\")\n",
    "\n",
    "# Convert to sktime panel format (MultiIndex)\n",
    "print(\"\\nConverting to sktime panel format...\")\n",
    "panel_multiindex = panel_df.set_index([\"region\", \"date\"])[\"sales\"]\n",
    "\n",
    "print(f\"Panel type: {type(panel_multiindex)}\")\n",
    "print(f\"Scitype: {scitype(panel_multiindex)}\")\n",
    "print(f\"Mtype: {mtype(panel_multiindex)}\")\n",
    "print(f\"Shape: {panel_multiindex.shape}\")\n",
    "print(f\"Index levels: {panel_multiindex.index.names}\")\n",
    "print(f\"Sample:\\n{panel_multiindex.head(10)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c81674",
   "metadata": {},
   "source": [
    "## 3. Handling Different Data Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a461c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate different data source scenarios\n",
    "print(\"Handling different data source formats:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Scenario 1: Wide format data (columns are time periods)\n",
    "print(\"\\n1. Wide format data:\")\n",
    "wide_data = pd.DataFrame(\n",
    "    {\n",
    "        \"series_id\": [\"A\", \"B\", \"C\"],\n",
    "        \"2020-01\": [100, 150, 80],\n",
    "        \"2020-02\": [105, 145, 85],\n",
    "        \"2020-03\": [110, 140, 90],\n",
    "        \"2020-04\": [108, 155, 88],\n",
    "    }\n",
    ")\n",
    "print(wide_data)\n",
    "\n",
    "# Convert wide to long format\n",
    "long_data = wide_data.melt(id_vars=[\"series_id\"], var_name=\"date\", value_name=\"value\")\n",
    "long_data[\"date\"] = pd.to_datetime(long_data[\"date\"])\n",
    "panel_from_wide = long_data.set_index([\"series_id\", \"date\"])[\"value\"]\n",
    "\n",
    "print(f\"Converted to panel format: {panel_from_wide.shape}\")\n",
    "print(panel_from_wide.head())\n",
    "\n",
    "# Scenario 2: JSON-like nested data\n",
    "print(\"\\n2. JSON-like nested data:\")\n",
    "json_like_data = {\n",
    "    \"series_A\": {\"2020-01-01\": 100, \"2020-01-02\": 102, \"2020-01-03\": 98},\n",
    "    \"series_B\": {\"2020-01-01\": 150, \"2020-01-02\": 155, \"2020-01-03\": 148},\n",
    "}\n",
    "\n",
    "# Convert JSON-like to sktime format\n",
    "json_data_list = []\n",
    "for series_id, series_data in json_like_data.items():\n",
    "    for date_str, value in series_data.items():\n",
    "        json_data_list.append(\n",
    "            {\"series_id\": series_id, \"date\": pd.to_datetime(date_str), \"value\": value}\n",
    "        )\n",
    "\n",
    "json_df = pd.DataFrame(json_data_list)\n",
    "panel_from_json = json_df.set_index([\"series_id\", \"date\"])[\"value\"]\n",
    "\n",
    "print(f\"Converted JSON data: {panel_from_json.shape}\")\n",
    "print(panel_from_json)\n",
    "\n",
    "# Scenario 3: Database-like format with separate date column\n",
    "print(\"\\n3. Database format with timestamps:\")\n",
    "db_data = pd.DataFrame(\n",
    "    {\n",
    "        \"timestamp\": pd.date_range(\"2020-01-01 09:00:00\", periods=24, freq=\"H\"),\n",
    "        \"sensor_id\": [\"temp_01\"] * 12 + [\"temp_02\"] * 12,\n",
    "        \"measurement\": np.random.normal(20, 2, 24),\n",
    "        \"location\": [\"Room_A\"] * 12 + [\"Room_B\"] * 12,\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Database-like data:\\n{db_data.head()}\")\n",
    "\n",
    "# Convert to time series format\n",
    "ts_from_db = db_data.set_index([\"sensor_id\", \"timestamp\"])[\"measurement\"]\n",
    "print(f\"\\nConverted to time series: {ts_from_db.shape}\")\n",
    "print(ts_from_db.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3c488a",
   "metadata": {},
   "source": [
    "## 4. Handling Common Data Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e05a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Common Data Issues and Solutions:\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Issue 1: Missing dates/irregular frequency\n",
    "print(\"\\n1. Missing dates and irregular frequency:\")\n",
    "irregular_dates = pd.to_datetime(\n",
    "    [\n",
    "        \"2020-01-01\",\n",
    "        \"2020-01-02\",\n",
    "        \"2020-01-05\",  # Missing 3rd and 4th\n",
    "        \"2020-01-06\",\n",
    "        \"2020-01-10\",  # Missing 7th, 8th, 9th\n",
    "    ]\n",
    ")\n",
    "irregular_values = [100, 102, 105, 108, 115]\n",
    "irregular_series = pd.Series(irregular_values, index=irregular_dates)\n",
    "\n",
    "print(f\"Original irregular series:\\n{irregular_series}\")\n",
    "\n",
    "# Solution: Reindex to regular frequency\n",
    "regular_index = pd.date_range(\n",
    "    start=irregular_series.index.min(), end=irregular_series.index.max(), freq=\"D\"\n",
    ")\n",
    "regular_series = irregular_series.reindex(regular_index)\n",
    "\n",
    "print(f\"\\nAfter reindexing (with NaN for missing):\\n{regular_series}\")\n",
    "\n",
    "# Fill missing values\n",
    "filled_series = regular_series.interpolate(method=\"linear\")\n",
    "print(f\"\\nAfter interpolation:\\n{filled_series}\")\n",
    "\n",
    "# Issue 2: Wrong data types\n",
    "print(\"\\n\\n2. Wrong data types:\")\n",
    "wrong_types = pd.DataFrame(\n",
    "    {\n",
    "        \"date\": [\"2020-01-01\", \"2020-01-02\", \"2020-01-03\"],  # String dates\n",
    "        \"value\": [\"100.5\", \"102.3\", \"99.8\"],  # String numbers\n",
    "        \"category\": [1, 2, 1],  # Numeric categories\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Original data types:\\n{wrong_types.dtypes}\")\n",
    "print(wrong_types)\n",
    "\n",
    "# Solution: Convert data types\n",
    "corrected_types = wrong_types.copy()\n",
    "corrected_types[\"date\"] = pd.to_datetime(corrected_types[\"date\"])\n",
    "corrected_types[\"value\"] = pd.to_numeric(corrected_types[\"value\"])\n",
    "corrected_types[\"category\"] = corrected_types[\"category\"].astype(\"category\")\n",
    "\n",
    "print(f\"\\nCorrected data types:\\n{corrected_types.dtypes}\")\n",
    "\n",
    "# Convert to time series\n",
    "corrected_series = corrected_types.set_index(\"date\")[\"value\"]\n",
    "print(f\"\\nFinal time series:\\n{corrected_series}\")\n",
    "\n",
    "# Issue 3: Duplicate timestamps\n",
    "print(\"\\n\\n3. Duplicate timestamps:\")\n",
    "duplicate_data = pd.DataFrame(\n",
    "    {\n",
    "        \"date\": [\"2020-01-01\", \"2020-01-01\", \"2020-01-02\", \"2020-01-02\"],\n",
    "        \"value\": [100, 105, 102, 98],\n",
    "        \"source\": [\"A\", \"B\", \"A\", \"B\"],\n",
    "    }\n",
    ")\n",
    "duplicate_data[\"date\"] = pd.to_datetime(duplicate_data[\"date\"])\n",
    "\n",
    "print(f\"Data with duplicates:\\n{duplicate_data}\")\n",
    "\n",
    "# Solution 1: Aggregate duplicates\n",
    "aggregated = duplicate_data.groupby(\"date\")[\"value\"].mean()\n",
    "print(f\"\\nAfter aggregation (mean):\\n{aggregated}\")\n",
    "\n",
    "# Solution 2: Keep source information\n",
    "multi_series = duplicate_data.set_index([\"source\", \"date\"])[\"value\"]\n",
    "print(f\"\\nAs panel data:\\n{multi_series}\")\n",
    "\n",
    "# Issue 4: Different frequencies in same dataset\n",
    "print(\"\\n\\n4. Mixed frequencies:\")\n",
    "daily_data = pd.DataFrame(\n",
    "    {\n",
    "        \"date\": pd.date_range(\"2020-01-01\", periods=7, freq=\"D\"),\n",
    "        \"daily_sales\": np.random.randint(100, 200, 7),\n",
    "    }\n",
    ")\n",
    "\n",
    "weekly_data = pd.DataFrame(\n",
    "    {\n",
    "        \"date\": pd.date_range(\"2020-01-01\", periods=4, freq=\"W\"),\n",
    "        \"weekly_budget\": np.random.randint(1000, 1500, 4),\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Daily data:\\n{daily_data}\")\n",
    "print(f\"\\nWeekly data:\\n{weekly_data}\")\n",
    "\n",
    "# Solution: Align to common frequency\n",
    "# Upsample weekly to daily\n",
    "weekly_series = weekly_data.set_index(\"date\")[\"weekly_budget\"]\n",
    "weekly_daily = weekly_series.resample(\"D\").ffill()  # Forward fill\n",
    "\n",
    "print(f\"\\nWeekly data upsampled to daily:\\n{weekly_daily}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1611acaf",
   "metadata": {},
   "source": [
    "## 5. Creating Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041f088f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_univariate_series(data, date_col, value_col, date_format=None):\n",
    "    \"\"\"Load univariate time series from DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pd.DataFrame or str\n",
    "        DataFrame or path to CSV file\n",
    "    date_col : str\n",
    "        Name of date column\n",
    "    value_col : str\n",
    "        Name of value column\n",
    "    date_format : str, optional\n",
    "        Date format string\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.Series\n",
    "        Time series in sktime format\n",
    "    \"\"\"\n",
    "    if isinstance(data, str):\n",
    "        df = pd.read_csv(data)\n",
    "    else:\n",
    "        df = data.copy()\n",
    "\n",
    "    # Convert date column\n",
    "    if date_format:\n",
    "        df[date_col] = pd.to_datetime(df[date_col], format=date_format)\n",
    "    else:\n",
    "        df[date_col] = pd.to_datetime(df[date_col])\n",
    "\n",
    "    # Convert to time series\n",
    "    series = df.set_index(date_col)[value_col]\n",
    "\n",
    "    # Validate\n",
    "    check_raise(series, mtype=\"pd.Series\", scitype=\"Series\")\n",
    "\n",
    "    return series\n",
    "\n",
    "\n",
    "def load_panel_data(data, instance_col, date_col, value_col, date_format=None):\n",
    "    \"\"\"Load panel data from DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pd.DataFrame or str\n",
    "        DataFrame or path to CSV file\n",
    "    instance_col : str\n",
    "        Name of instance/series identifier column\n",
    "    date_col : str\n",
    "        Name of date column\n",
    "    value_col : str\n",
    "        Name of value column\n",
    "    date_format : str, optional\n",
    "        Date format string\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.Series\n",
    "        Panel data in sktime format (MultiIndex)\n",
    "    \"\"\"\n",
    "    if isinstance(data, str):\n",
    "        df = pd.read_csv(data)\n",
    "    else:\n",
    "        df = data.copy()\n",
    "\n",
    "    # Convert date column\n",
    "    if date_format:\n",
    "        df[date_col] = pd.to_datetime(df[date_col], format=date_format)\n",
    "    else:\n",
    "        df[date_col] = pd.to_datetime(df[date_col])\n",
    "\n",
    "    # Convert to panel format\n",
    "    panel = df.set_index([instance_col, date_col])[value_col]\n",
    "\n",
    "    # Validate\n",
    "    check_raise(panel, mtype=\"pd-multiindex\", scitype=\"Panel\")\n",
    "\n",
    "    return panel\n",
    "\n",
    "\n",
    "def clean_time_series(series, freq=None, fill_method=\"interpolate\"):\n",
    "    \"\"\"Clean and regularize time series.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    series : pd.Series\n",
    "        Input time series\n",
    "    freq : str, optional\n",
    "        Target frequency (e.g., 'D', 'H', 'M')\n",
    "    fill_method : str, default='interpolate'\n",
    "        Method to fill missing values\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.Series\n",
    "        Cleaned time series\n",
    "    \"\"\"\n",
    "    cleaned = series.copy()\n",
    "\n",
    "    # Remove duplicates (keep first)\n",
    "    cleaned = cleaned[~cleaned.index.duplicated(keep=\"first\")]\n",
    "\n",
    "    # Sort by index\n",
    "    cleaned = cleaned.sort_index()\n",
    "\n",
    "    # Regularize frequency if specified\n",
    "    if freq:\n",
    "        regular_index = pd.date_range(\n",
    "            start=cleaned.index.min(), end=cleaned.index.max(), freq=freq\n",
    "        )\n",
    "        cleaned = cleaned.reindex(regular_index)\n",
    "\n",
    "    # Fill missing values\n",
    "    if fill_method == \"interpolate\":\n",
    "        cleaned = cleaned.interpolate()\n",
    "    elif fill_method == \"ffill\":\n",
    "        cleaned = cleaned.fillna(method=\"ffill\")\n",
    "    elif fill_method == \"bfill\":\n",
    "        cleaned = cleaned.fillna(method=\"bfill\")\n",
    "\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "# Test the helper functions\n",
    "print(\"Testing helper functions:\")\n",
    "print(\"=\" * 25)\n",
    "\n",
    "# Test univariate loader\n",
    "test_data = pd.DataFrame(\n",
    "    {\n",
    "        \"timestamp\": [\"2020-01-01\", \"2020-01-02\", \"2020-01-03\"],\n",
    "        \"sales\": [100, 105, 98],\n",
    "        \"region\": [\"A\", \"A\", \"A\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "series_result = load_univariate_series(test_data, \"timestamp\", \"sales\")\n",
    "print(\"\\nUnivariate series loaded:\")\n",
    "print(series_result)\n",
    "\n",
    "# Test panel loader\n",
    "panel_test_data = pd.DataFrame(\n",
    "    {\n",
    "        \"series_id\": [\"A\", \"A\", \"B\", \"B\"],\n",
    "        \"date\": [\"2020-01-01\", \"2020-01-02\", \"2020-01-01\", \"2020-01-02\"],\n",
    "        \"value\": [100, 105, 90, 92],\n",
    "    }\n",
    ")\n",
    "\n",
    "panel_result = load_panel_data(panel_test_data, \"series_id\", \"date\", \"value\")\n",
    "print(\"\\nPanel data loaded:\")\n",
    "print(panel_result)\n",
    "\n",
    "# Test cleaning function\n",
    "messy_series = pd.Series(\n",
    "    [100, np.nan, 105, 110],\n",
    "    index=pd.to_datetime([\"2020-01-01\", \"2020-01-03\", \"2020-01-04\", \"2020-01-06\"]),\n",
    ")\n",
    "\n",
    "cleaned_result = clean_time_series(messy_series, freq=\"D\")\n",
    "print(\"\\nCleaned series:\")\n",
    "print(cleaned_result)\n",
    "\n",
    "print(\"\\n✓ All helper functions working correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759564e6",
   "metadata": {},
   "source": [
    "## 6. Real-World Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e455fcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Real-World Data Loading Examples:\")\n",
    "print(\"=\" * 34)\n",
    "\n",
    "# Example 1: Stock price data\n",
    "print(\"\\n1. Stock Price Data Format:\")\n",
    "stock_data = pd.DataFrame(\n",
    "    {\n",
    "        \"Date\": pd.date_range(\"2020-01-01\", periods=10, freq=\"B\"),  # Business days\n",
    "        \"Open\": np.random.uniform(100, 110, 10),\n",
    "        \"High\": np.random.uniform(110, 120, 10),\n",
    "        \"Low\": np.random.uniform(90, 100, 10),\n",
    "        \"Close\": np.random.uniform(95, 115, 10),\n",
    "        \"Volume\": np.random.randint(1000000, 5000000, 10),\n",
    "    }\n",
    ")\n",
    "\n",
    "print(stock_data.head())\n",
    "\n",
    "# Convert to time series for closing prices\n",
    "close_prices = stock_data.set_index(\"Date\")[\"Close\"]\n",
    "print(f\"\\nClose prices as time series: {scitype(close_prices)}\")\n",
    "\n",
    "# Example 2: IoT sensor data\n",
    "print(\"\\n\\n2. IoT Sensor Data Format:\")\n",
    "iot_data = pd.DataFrame(\n",
    "    {\n",
    "        \"timestamp\": pd.date_range(\"2020-01-01 00:00:00\", periods=48, freq=\"30min\"),\n",
    "        \"device_id\": [\"sensor_01\"] * 24 + [\"sensor_02\"] * 24,\n",
    "        \"temperature\": np.random.normal(22, 3, 48),\n",
    "        \"humidity\": np.random.normal(60, 10, 48),\n",
    "        \"battery_level\": np.random.uniform(20, 100, 48),\n",
    "    }\n",
    ")\n",
    "\n",
    "print(iot_data.head())\n",
    "\n",
    "# Convert to panel data for temperature readings\n",
    "temp_panel = iot_data.set_index([\"device_id\", \"timestamp\"])[\"temperature\"]\n",
    "print(f\"\\nTemperature panel data: {scitype(temp_panel)}\")\n",
    "print(f\"Shape: {temp_panel.shape}\")\n",
    "\n",
    "# Example 3: Sales data with hierarchy\n",
    "print(\"\\n\\n3. Hierarchical Sales Data:\")\n",
    "sales_data = pd.DataFrame(\n",
    "    {\n",
    "        \"date\": pd.date_range(\"2020-01-01\", periods=12, freq=\"M\"),\n",
    "        \"country\": [\"USA\"] * 6 + [\"Canada\"] * 6,\n",
    "        \"state\": [\"CA\", \"NY\", \"TX\"] * 2 + [\"ON\", \"BC\"] * 3,\n",
    "        \"product\": [\"A\", \"B\"] * 6,\n",
    "        \"sales\": np.random.randint(1000, 5000, 12),\n",
    "    }\n",
    ")\n",
    "\n",
    "print(sales_data.head())\n",
    "\n",
    "# Different grouping strategies\n",
    "# By country\n",
    "country_sales = sales_data.groupby([\"country\", \"date\"])[\"sales\"].sum().reset_index()\n",
    "country_panel = country_sales.set_index([\"country\", \"date\"])[\"sales\"]\n",
    "print(f\"\\nCountry-level panel: {country_panel.shape}\")\n",
    "\n",
    "# By product\n",
    "product_sales = sales_data.groupby([\"product\", \"date\"])[\"sales\"].sum().reset_index()\n",
    "product_panel = product_sales.set_index([\"product\", \"date\"])[\"sales\"]\n",
    "print(f\"Product-level panel: {product_panel.shape}\")\n",
    "\n",
    "# Example 4: Web analytics data\n",
    "print(\"\\n\\n4. Web Analytics Data:\")\n",
    "web_data = pd.DataFrame(\n",
    "    {\n",
    "        \"date\": pd.date_range(\"2020-01-01\", periods=30, freq=\"D\"),\n",
    "        \"page_views\": np.random.poisson(1000, 30),\n",
    "        \"unique_visitors\": np.random.poisson(500, 30),\n",
    "        \"bounce_rate\": np.random.uniform(0.3, 0.7, 30),\n",
    "        \"conversion_rate\": np.random.uniform(0.01, 0.05, 30),\n",
    "    }\n",
    ")\n",
    "\n",
    "print(web_data.head())\n",
    "\n",
    "# Convert multiple metrics to separate series\n",
    "page_views_ts = web_data.set_index(\"date\")[\"page_views\"]\n",
    "conversion_ts = web_data.set_index(\"date\")[\"conversion_rate\"]\n",
    "\n",
    "print(f\"\\nPage views time series: {page_views_ts.shape}\")\n",
    "print(f\"Conversion rate time series: {conversion_ts.shape}\")\n",
    "\n",
    "# Combine into multivariate series if needed\n",
    "multivariate_df = web_data.set_index(\"date\")[[\"page_views\", \"conversion_rate\"]]\n",
    "print(f\"\\nMultivariate data: {multivariate_df.shape}\")\n",
    "print(f\"Scitype: {scitype(multivariate_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e023932b",
   "metadata": {},
   "source": [
    "## 7. Validation and Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63844938",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_time_series_data(data, name=\"data\"):\n",
    "    \"\"\"Comprehensive validation of time series data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pd.Series or pd.DataFrame\n",
    "        Time series data to validate\n",
    "    name : str\n",
    "        Name for reporting\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Validation report\n",
    "    \"\"\"\n",
    "    report = {\"name\": name, \"issues\": [], \"warnings\": [], \"info\": []}\n",
    "\n",
    "    # Basic type checks\n",
    "    if not isinstance(data, (pd.Series, pd.DataFrame)):\n",
    "        report[\"issues\"].append(f\"Data is not pandas Series or DataFrame: {type(data)}\")\n",
    "        return report\n",
    "\n",
    "    # Index checks\n",
    "    if not isinstance(data.index, (pd.DatetimeIndex, pd.MultiIndex)):\n",
    "        report[\"issues\"].append(\n",
    "            f\"Index is not DatetimeIndex or MultiIndex: {type(data.index)}\"\n",
    "        )\n",
    "\n",
    "    # Check for missing values\n",
    "    if (\n",
    "        data.isnull().any().any()\n",
    "        if isinstance(data, pd.DataFrame)\n",
    "        else data.isnull().any()\n",
    "    ):\n",
    "        missing_count = (\n",
    "            data.isnull().sum().sum()\n",
    "            if isinstance(data, pd.DataFrame)\n",
    "            else data.isnull().sum()\n",
    "        )\n",
    "        report[\"warnings\"].append(f\"Contains {missing_count} missing values\")\n",
    "\n",
    "    # Check for duplicates in index\n",
    "    if data.index.duplicated().any():\n",
    "        dup_count = data.index.duplicated().sum()\n",
    "        report[\"warnings\"].append(f\"Contains {dup_count} duplicate index values\")\n",
    "\n",
    "    # Check data types\n",
    "    if isinstance(data, pd.Series):\n",
    "        if not pd.api.types.is_numeric_dtype(data):\n",
    "            report[\"warnings\"].append(f\"Series is not numeric: {data.dtype}\")\n",
    "    else:\n",
    "        non_numeric = [\n",
    "            col for col in data.columns if not pd.api.types.is_numeric_dtype(data[col])\n",
    "        ]\n",
    "        if non_numeric:\n",
    "            report[\"warnings\"].append(f\"Non-numeric columns: {non_numeric}\")\n",
    "\n",
    "    # Check frequency regularity (for DatetimeIndex)\n",
    "    if isinstance(data.index, pd.DatetimeIndex):\n",
    "        try:\n",
    "            freq = pd.infer_freq(data.index)\n",
    "            if freq is None:\n",
    "                report[\"warnings\"].append(\"Could not infer regular frequency\")\n",
    "            else:\n",
    "                report[\"info\"].append(f\"Inferred frequency: {freq}\")\n",
    "        except Exception:\n",
    "            report[\"warnings\"].append(\"Error inferring frequency\")\n",
    "\n",
    "    # Size checks\n",
    "    report[\"info\"].append(f\"Shape: {data.shape}\")\n",
    "    report[\"info\"].append(f\"Date range: {data.index.min()} to {data.index.max()}\")\n",
    "\n",
    "    # sktime compatibility\n",
    "    try:\n",
    "        check_raise(data, mtype=mtype(data), scitype=scitype(data))\n",
    "        report[\"info\"].append(f\"✓ sktime compatible: {scitype(data)}/{mtype(data)}\")\n",
    "    except Exception as e:\n",
    "        report[\"issues\"].append(f\"Not sktime compatible: {str(e)[:100]}\")\n",
    "\n",
    "    return report\n",
    "\n",
    "\n",
    "def print_validation_report(report):\n",
    "    \"\"\"Print formatted validation report.\"\"\"\n",
    "    print(f\"\\nValidation Report for '{report['name']}':\")\n",
    "    print(\"=\" * (len(report[\"name\"]) + 25))\n",
    "\n",
    "    if report[\"issues\"]:\n",
    "        print(\"\\n❌ ISSUES:\")\n",
    "        for issue in report[\"issues\"]:\n",
    "            print(f\"   • {issue}\")\n",
    "\n",
    "    if report[\"warnings\"]:\n",
    "        print(\"\\n⚠️  WARNINGS:\")\n",
    "        for warning in report[\"warnings\"]:\n",
    "            print(f\"   • {warning}\")\n",
    "\n",
    "    if report[\"info\"]:\n",
    "        print(\"\\nℹ️  INFO:\")\n",
    "        for info in report[\"info\"]:\n",
    "            print(f\"   • {info}\")\n",
    "\n",
    "    # Overall status\n",
    "    if not report[\"issues\"]:\n",
    "        if not report[\"warnings\"]:\n",
    "            print(\"\\n✅ Status: EXCELLENT - No issues or warnings\")\n",
    "        else:\n",
    "            print(\"\\n✅ Status: GOOD - No critical issues, some warnings\")\n",
    "    else:\n",
    "        print(\"\\n❌ Status: NEEDS ATTENTION - Critical issues found\")\n",
    "\n",
    "\n",
    "# Test validation function\n",
    "print(\"Testing data validation:\")\n",
    "\n",
    "# Good data\n",
    "good_data = pd.Series(\n",
    "    np.random.randn(30),\n",
    "    index=pd.date_range(\"2020-01-01\", periods=30, freq=\"D\"),\n",
    "    name=\"good_series\",\n",
    ")\n",
    "\n",
    "good_report = validate_time_series_data(good_data, \"Good Time Series\")\n",
    "print_validation_report(good_report)\n",
    "\n",
    "# Problematic data\n",
    "bad_data = pd.Series(\n",
    "    [100, np.nan, \"invalid\", 105, 102],\n",
    "    index=[1, 2, 2, 3, 4],  # Non-datetime index with duplicates\n",
    "    name=\"bad_series\",\n",
    ")\n",
    "\n",
    "bad_report = validate_time_series_data(bad_data, \"Problematic Data\")\n",
    "print_validation_report(bad_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0f9491",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This guide covered comprehensive data loading and formatting for sktime:\n",
    "\n",
    "### Key Topics Covered:\n",
    "\n",
    "1. **CSV Data Loading**: Converting flat files to time series format\n",
    "2. **Panel Data**: Handling multiple time series with MultiIndex\n",
    "3. **Different Sources**: Wide format, JSON-like, database formats\n",
    "4. **Common Issues**: Missing dates, wrong types, duplicates, mixed frequencies\n",
    "5. **Helper Functions**: Reusable utilities for data loading\n",
    "6. **Real-World Examples**: Stock, IoT, sales, web analytics data\n",
    "7. **Validation**: Comprehensive data quality checking\n",
    "\n",
    "### Best Practices:\n",
    "\n",
    "- **Always validate** data after loading\n",
    "- **Use DatetimeIndex** for time series data\n",
    "- **Handle missing values** appropriately\n",
    "- **Check data types** and convert as needed\n",
    "- **Use MultiIndex** for panel data\n",
    "- **Create reusable functions** for common patterns\n",
    "- **Test with sktime compatibility** functions\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- Apply these techniques to your own data\n",
    "- Explore \"Handle Missing Values\" guide for advanced missing data techniques\n",
    "- Learn \"Convert Between Data Formats\" for more transformation options\n",
    "- Try forecasting tutorials with your formatted data"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
