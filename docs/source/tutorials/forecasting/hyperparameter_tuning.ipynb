{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8ba19a4",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning for Time Series Forecasting\n",
    "\n",
    "This tutorial demonstrates how to optimize forecaster parameters using sktime's tuning capabilities.\n",
    "\n",
    "**Duration:** ~10 minutes\n",
    "\n",
    "## Learning objectives\n",
    "\n",
    "By the end of this tutorial, you will be able to:\n",
    "- Use tuners in sktime for hyperparameter optimization\n",
    "- Tune parameters of simple models\n",
    "- Optimize complex pipeline compositions\n",
    "- Perform cross-validation of tuned models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977ce1d1",
   "metadata": {},
   "source": [
    "## 1. Introduction to Tuners in sktime\n",
    "\n",
    "sktime provides several tuning strategies for optimizing forecaster parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21971de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sktime.datasets import load_airline\n",
    "from sktime.forecasting.model_evaluation import evaluate\n",
    "from sktime.split import ExpandingWindowSplitter\n",
    "from sktime.utils.plotting import plot_series\n",
    "from sktime.performance_metrics.forecasting import mean_absolute_percentage_error\n",
    "\n",
    "# Load data\n",
    "y = load_airline()\n",
    "print(f\"Dataset: {y.shape[0]} observations from {y.index[0]} to {y.index[-1]}\")\n",
    "\n",
    "# Split data for final evaluation\n",
    "y_train = y.iloc[:-12]\n",
    "y_test = y.iloc[-12:]\n",
    "\n",
    "print(f\"Training: {len(y_train)} observations\")\n",
    "print(f\"Test: {len(y_test)} observations\")\n",
    "\n",
    "# Plot the data\n",
    "plot_series(y_train, y_test, labels=[\"Training\", \"Test\"], title=\"Airline Dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b411f015",
   "metadata": {},
   "source": [
    "## 2. Tuning a Simple Model\n",
    "\n",
    "Let's start by tuning the parameters of an exponential smoothing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3541777",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.forecasting.exp_smoothing import ExponentialSmoothing\n",
    "from sktime.forecasting.model_selection import ForecastingGridSearchCV\n",
    "\n",
    "# Define the base forecaster\n",
    "base_forecaster = ExponentialSmoothing(sp=12)\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    \"trend\": [None, \"add\", \"mul\"],\n",
    "    \"seasonal\": [None, \"add\", \"mul\"],\n",
    "    \"damped_trend\": [True, False]\n",
    "}\n",
    "\n",
    "print(f\"Parameter grid combinations: {len(param_grid['trend']) * len(param_grid['seasonal']) * len(param_grid['damped_trend'])}\")\n",
    "print(f\"Parameters to tune: {list(param_grid.keys())}\")\n",
    "\n",
    "# Set up cross-validation\n",
    "cv = ExpandingWindowSplitter(\n",
    "    initial_window=60,\n",
    "    step_length=12,\n",
    "    fh=[1, 3, 6, 12]\n",
    ")\n",
    "\n",
    "print(f\"\\nCV setup: {cv.get_n_splits(y_train)} folds\")\n",
    "print(f\"Forecast horizons: {cv.fh}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3c2a5d",
   "metadata": {},
   "source": [
    "### 2.1 Perform Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7922af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create grid search tuner\n",
    "tuner = ForecastingGridSearchCV(\n",
    "    forecaster=base_forecaster,\n",
    "    cv=cv,\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"mean_absolute_percentage_error\",\n",
    "    n_jobs=1,  # Set to -1 for parallel processing\n",
    "    refit=True\n",
    ")\n",
    "\n",
    "print(\"Running grid search...\")\n",
    "print(\"This may take a moment...\")\n",
    "\n",
    "# Fit the tuner\n",
    "tuner.fit(y_train)\n",
    "\n",
    "print(\"Grid search completed!\")\n",
    "print(f\"Best parameters: {tuner.best_params_}\")\n",
    "print(f\"Best score (MAPE): {tuner.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67639448",
   "metadata": {},
   "source": [
    "### 2.2 Analyze Tuning Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8654645b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get detailed results\n",
    "results_df = pd.DataFrame(tuner.cv_results_)\n",
    "\n",
    "print(\"Top 5 parameter combinations:\")\n",
    "top_results = results_df.nsmallest(5, 'mean_test_score')[[\n",
    "    'param_trend', 'param_seasonal', 'param_damped_trend', \n",
    "    'mean_test_score', 'std_test_score'\n",
    "]]\n",
    "print(top_results)\n",
    "\n",
    "# Visualize parameter importance\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Trend effect\n",
    "trend_scores = results_df.groupby('param_trend')['mean_test_score'].mean()\n",
    "trend_scores.plot(kind='bar', ax=axes[0], title='Effect of Trend Parameter')\n",
    "axes[0].set_ylabel('Mean MAPE')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Seasonal effect\n",
    "seasonal_scores = results_df.groupby('param_seasonal')['mean_test_score'].mean()\n",
    "seasonal_scores.plot(kind='bar', ax=axes[1], title='Effect of Seasonal Parameter')\n",
    "axes[1].set_ylabel('Mean MAPE')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Damped trend effect\n",
    "damped_scores = results_df.groupby('param_damped_trend')['mean_test_score'].mean()\n",
    "damped_scores.plot(kind='bar', ax=axes[2], title='Effect of Damped Trend')\n",
    "axes[2].set_ylabel('Mean MAPE')\n",
    "axes[2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae02ae6",
   "metadata": {},
   "source": [
    "## 3. Tuning Pipeline Compositions\n",
    "\n",
    "Now let's tune a more complex pipeline with transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07752037",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.forecasting.compose import TransformedTargetForecaster\n",
    "from sktime.transformations.series.boxcox import BoxCoxTransformer\n",
    "from sktime.transformations.series.detrend import Detrender\n",
    "from sktime.transformations.compose import TransformerPipeline\n",
    "from sktime.forecasting.naive import NaiveForecaster\n",
    "\n",
    "# Create a pipeline with transformations\n",
    "transformer_pipeline = TransformerPipeline([\n",
    "    (\"boxcox\", BoxCoxTransformer()),\n",
    "    (\"detrend\", Detrender())\n",
    "])\n",
    "\n",
    "pipeline_forecaster = TransformedTargetForecaster([\n",
    "    (\"transformer\", transformer_pipeline),\n",
    "    (\"forecaster\", ExponentialSmoothing(sp=12))\n",
    "])\n",
    "\n",
    "# Define pipeline parameter grid\n",
    "pipeline_param_grid = {\n",
    "    # BoxCox parameters\n",
    "    \"steps__transformer__steps__boxcox__method\": [\"mle\", \"pearsonr\"],\n",
    "    \n",
    "    # Forecaster parameters\n",
    "    \"steps__forecaster__trend\": [None, \"add\"],\n",
    "    \"steps__forecaster__seasonal\": [\"add\", \"mul\"],\n",
    "    \"steps__forecaster__damped_trend\": [True, False]\n",
    "}\n",
    "\n",
    "print(f\"Pipeline parameter combinations: {2 * 2 * 2 * 2}\")\n",
    "print(\"Parameters to tune:\")\n",
    "for param, values in pipeline_param_grid.items():\n",
    "    print(f\"  {param}: {values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae74c420",
   "metadata": {},
   "source": [
    "### 3.1 Tune the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20ff980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline tuner\n",
    "pipeline_tuner = ForecastingGridSearchCV(\n",
    "    forecaster=pipeline_forecaster,\n",
    "    cv=cv,\n",
    "    param_grid=pipeline_param_grid,\n",
    "    scoring=\"mean_absolute_percentage_error\",\n",
    "    n_jobs=1,\n",
    "    refit=True\n",
    ")\n",
    "\n",
    "print(\"Running pipeline tuning...\")\n",
    "pipeline_tuner.fit(y_train)\n",
    "\n",
    "print(\"Pipeline tuning completed!\")\n",
    "print(f\"\\nBest pipeline parameters:\")\n",
    "for param, value in pipeline_tuner.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "print(f\"\\nBest score (MAPE): {pipeline_tuner.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29834890",
   "metadata": {},
   "source": [
    "## 4. Comparing Tuned Models\n",
    "\n",
    "Let's compare the performance of our tuned models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acc2565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create baseline model (no tuning)\n",
    "baseline = ExponentialSmoothing(trend=\"add\", seasonal=\"mul\", sp=12)\n",
    "baseline.fit(y_train)\n",
    "y_pred_baseline = baseline.predict(fh=range(1, 13))\n",
    "\n",
    "# Get predictions from tuned models\n",
    "y_pred_tuned = tuner.predict(fh=range(1, 13))\n",
    "y_pred_pipeline = pipeline_tuner.predict(fh=range(1, 13))\n",
    "\n",
    "# Calculate performance metrics\n",
    "mape_baseline = mean_absolute_percentage_error(y_test, y_pred_baseline)\n",
    "mape_tuned = mean_absolute_percentage_error(y_test, y_pred_tuned)\n",
    "mape_pipeline = mean_absolute_percentage_error(y_test, y_pred_pipeline)\n",
    "\n",
    "print(\"Model Comparison:\")\n",
    "print(f\"Baseline (no tuning):    {mape_baseline:.4f}\")\n",
    "print(f\"Tuned model:             {mape_tuned:.4f}\")\n",
    "print(f\"Tuned pipeline:          {mape_pipeline:.4f}\")\n",
    "\n",
    "print(f\"\\nImprovement over baseline:\")\n",
    "print(f\"Tuned model:    {((mape_baseline - mape_tuned) / mape_baseline * 100):+.1f}%\")\n",
    "print(f\"Tuned pipeline: {((mape_baseline - mape_pipeline) / mape_baseline * 100):+.1f}%\")\n",
    "\n",
    "# Plot results\n",
    "plot_series(\n",
    "    y_train.iloc[-24:], y_test, \n",
    "    y_pred_baseline, y_pred_tuned, y_pred_pipeline,\n",
    "    labels=[\"Training\", \"Actual\", \"Baseline\", \"Tuned\", \"Tuned Pipeline\"],\n",
    "    title=\"Comparison of Tuned Models\"\n",
    ")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69239dd2",
   "metadata": {},
   "source": [
    "## 5. Cross-validation of Tuned Models\n",
    "\n",
    "Let's perform a more robust evaluation using cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534cdb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up evaluation CV (different from tuning CV)\n",
    "eval_cv = ExpandingWindowSplitter(\n",
    "    initial_window=72,\n",
    "    step_length=6,\n",
    "    fh=[1, 6, 12]\n",
    ")\n",
    "\n",
    "print(f\"Evaluation CV: {eval_cv.get_n_splits(y)} folds\")\n",
    "\n",
    "# Models to evaluate\n",
    "models = {\n",
    "    \"Baseline\": ExponentialSmoothing(trend=\"add\", seasonal=\"mul\", sp=12),\n",
    "    \"Tuned_Simple\": tuner.best_estimator_,\n",
    "    \"Tuned_Pipeline\": pipeline_tuner.best_estimator_\n",
    "}\n",
    "\n",
    "# Evaluate all models\n",
    "print(\"\\nRunning cross-validation evaluation...\")\n",
    "cv_results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Evaluating {name}...\")\n",
    "    result = evaluate(\n",
    "        forecaster=model,\n",
    "        y=y,\n",
    "        cv=eval_cv,\n",
    "        scoring=[\"mean_absolute_percentage_error\", \"mean_absolute_error\"],\n",
    "        return_data=False\n",
    "    )\n",
    "    cv_results[name] = result\n",
    "\n",
    "print(\"Cross-validation completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572160d1",
   "metadata": {},
   "source": [
    "### 5.1 Analyze Cross-validation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8755ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze CV results\n",
    "print(\"Cross-validation Results:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "summary_stats = []\n",
    "for name, results in cv_results.items():\n",
    "    if isinstance(results, dict):\n",
    "        # Convert to DataFrame if needed\n",
    "        results_df = pd.DataFrame(results)\n",
    "    else:\n",
    "        results_df = results\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    \n",
    "    # Calculate summary statistics\n",
    "    summary = results_df.describe()\n",
    "    print(summary)\n",
    "    \n",
    "    # Store for comparison\n",
    "    mean_mape = results_df['test_mean_absolute_percentage_error'].mean()\n",
    "    std_mape = results_df['test_mean_absolute_percentage_error'].std()\n",
    "    summary_stats.append({\n",
    "        'Model': name,\n",
    "        'Mean_MAPE': mean_mape,\n",
    "        'Std_MAPE': std_mape,\n",
    "        'CV_Score': mean_mape  # For ranking\n",
    "    })\n",
    "\n",
    "# Create comparison table\n",
    "comparison_df = pd.DataFrame(summary_stats)\n",
    "comparison_df = comparison_df.sort_values('CV_Score')\n",
    "\n",
    "print(\"\\n\\nMODEL RANKING (by Cross-validation MAPE):\")\n",
    "print(\"=\" * 45)\n",
    "for i, row in comparison_df.iterrows():\n",
    "    print(f\"{row['Model']:15}: {row['Mean_MAPE']:6.4f} ± {row['Std_MAPE']:6.4f}\")\n",
    "\n",
    "# Statistical significance test (simple)\n",
    "best_model = comparison_df.iloc[0]['Model']\n",
    "print(f\"\\nBest model: {best_model}\")\n",
    "\n",
    "# Plot CV results\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "models_list = []\n",
    "mape_values = []\n",
    "\n",
    "for name, results in cv_results.items():\n",
    "    if isinstance(results, dict):\n",
    "        results_df = pd.DataFrame(results)\n",
    "    else:\n",
    "        results_df = results\n",
    "    \n",
    "    mapes = results_df['test_mean_absolute_percentage_error']\n",
    "    models_list.extend([name] * len(mapes))\n",
    "    mape_values.extend(mapes)\n",
    "\n",
    "# Create box plot\n",
    "cv_plot_df = pd.DataFrame({'Model': models_list, 'MAPE': mape_values})\n",
    "cv_plot_df.boxplot(column='MAPE', by='Model', ax=ax)\n",
    "ax.set_title('Cross-validation MAPE Distribution')\n",
    "ax.set_xlabel('Model')\n",
    "ax.set_ylabel('MAPE')\n",
    "plt.suptitle('')  # Remove automatic title\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a75119",
   "metadata": {},
   "source": [
    "## 6. Advanced Tuning Strategies\n",
    "\n",
    "Beyond grid search, there are other tuning approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b0b964",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Advanced Tuning Strategies:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "print(\"\\n1. RANDOMIZED SEARCH:\")\n",
    "print(\"   • Samples random combinations from parameter space\")\n",
    "print(\"   • More efficient for large parameter spaces\")\n",
    "print(\"   • Good for continuous parameters\")\n",
    "\n",
    "print(\"\\n2. BAYESIAN OPTIMIZATION:\")\n",
    "print(\"   • Uses previous evaluations to guide search\")\n",
    "print(\"   • More efficient than grid/random search\")\n",
    "print(\"   • Available through scikit-optimize integration\")\n",
    "\n",
    "print(\"\\n3. NESTED CROSS-VALIDATION:\")\n",
    "print(\"   • Inner loop: parameter tuning\")\n",
    "print(\"   • Outer loop: model evaluation\")\n",
    "print(\"   • Provides unbiased performance estimates\")\n",
    "\n",
    "# Demonstrate randomized search concept\n",
    "from sktime.forecasting.model_selection import ForecastingRandomizedSearchCV\n",
    "from scipy.stats import uniform\n",
    "\n",
    "print(\"\\n\\nExample: Randomized Search Setup\")\n",
    "print(\"(Not executed due to time constraints)\")\n",
    "\n",
    "# Example parameter distribution for randomized search\n",
    "random_param_dist = {\n",
    "    \"trend\": [None, \"add\", \"mul\"],\n",
    "    \"seasonal\": [None, \"add\", \"mul\"],\n",
    "    \"damped_trend\": [True, False],\n",
    "    # For models with continuous parameters:\n",
    "    # \"alpha\": uniform(0.01, 0.99),  # Uniform distribution between 0.01 and 1.0\n",
    "}\n",
    "\n",
    "print(f\"Randomized search would sample from: {random_param_dist}\")\n",
    "\n",
    "# Example setup (not executed)\n",
    "# random_tuner = ForecastingRandomizedSearchCV(\n",
    "#     forecaster=base_forecaster,\n",
    "#     cv=cv,\n",
    "#     param_distributions=random_param_dist,\n",
    "#     n_iter=20,  # Number of random samples\n",
    "#     scoring=\"mean_absolute_percentage_error\",\n",
    "#     random_state=42\n",
    "# )\n",
    "\n",
    "print(\"\\nAdvantages of each approach:\")\n",
    "print(\"Grid Search:      Exhaustive, good for small parameter spaces\")\n",
    "print(\"Random Search:    Efficient, good for large/continuous spaces\")\n",
    "print(\"Bayesian Opt:     Intelligent, good for expensive evaluations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8937211c",
   "metadata": {},
   "source": [
    "## 7. Best Practices for Hyperparameter Tuning\n",
    "\n",
    "Key guidelines for effective parameter optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a2f0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hyperparameter Tuning Best Practices:\")\n",
    "print(\"=\" * 38)\n",
    "\n",
    "print(\"\\n1. SEPARATE VALIDATION:\")\n",
    "print(\"   ✓ Use different CV for tuning vs. final evaluation\")\n",
    "print(\"   ✓ Never tune on your final test set\")\n",
    "print(\"   ✓ Consider nested CV for unbiased estimates\")\n",
    "\n",
    "print(\"\\n2. PARAMETER SPACE DESIGN:\")\n",
    "print(\"   ✓ Start with wide ranges, then narrow down\")\n",
    "print(\"   ✓ Use domain knowledge to guide ranges\")\n",
    "print(\"   ✓ Consider parameter interactions\")\n",
    "\n",
    "print(\"\\n3. COMPUTATIONAL EFFICIENCY:\")\n",
    "print(\"   ✓ Use parallel processing (n_jobs=-1)\")\n",
    "print(\"   ✓ Start with coarse grid, refine iteratively\")\n",
    "print(\"   ✓ Consider early stopping for expensive models\")\n",
    "\n",
    "print(\"\\n4. VALIDATION STRATEGY:\")\n",
    "print(\"   ✓ Ensure CV reflects real-world usage\")\n",
    "print(\"   ✓ Use multiple metrics for comprehensive evaluation\")\n",
    "print(\"   ✓ Check for overfitting to validation set\")\n",
    "\n",
    "print(\"\\n5. RESULT INTERPRETATION:\")\n",
    "print(\"   ✓ Analyze parameter importance\")\n",
    "print(\"   ✓ Check for parameter stability\")\n",
    "print(\"   ✓ Consider confidence intervals\")\n",
    "\n",
    "# Practical example of parameter analysis\n",
    "print(\"\\n\\nPractical Tips:\")\n",
    "print(\"=\" * 15)\n",
    "\n",
    "print(\"\\nParameter Stability Check:\")\n",
    "# Get top 3 parameter combinations\n",
    "if hasattr(tuner, 'cv_results_'):\n",
    "    results_df = pd.DataFrame(tuner.cv_results_)\n",
    "    top_3 = results_df.nsmallest(3, 'mean_test_score')\n",
    "    \n",
    "    print(\"Top 3 parameter combinations:\")\n",
    "    for i, (_, row) in enumerate(top_3.iterrows()):\n",
    "        score_diff = row['mean_test_score'] - results_df['mean_test_score'].min()\n",
    "        print(f\"{i+1}. Score: {row['mean_test_score']:.4f} (+{score_diff:.4f})\")\n",
    "        print(f\"   Parameters: trend={row['param_trend']}, \"\n",
    "              f\"seasonal={row['param_seasonal']}, \"\n",
    "              f\"damped={row['param_damped_trend']}\")\n",
    "    \n",
    "    score_range = results_df['mean_test_score'].max() - results_df['mean_test_score'].min()\n",
    "    print(f\"\\nScore range: {score_range:.4f}\")\n",
    "    if score_range < 0.001:\n",
    "        print(\"→ Parameters have small impact, focus on other aspects\")\n",
    "    else:\n",
    "        print(\"→ Parameter choice matters, tuning is worthwhile\")\n",
    "\n",
    "print(\"\\nCommon Pitfalls to Avoid:\")\n",
    "print(\"❌ Tuning on test set\")\n",
    "print(\"❌ Using same CV for tuning and evaluation\")\n",
    "print(\"❌ Over-interpreting small performance differences\")\n",
    "print(\"❌ Ignoring computational cost vs. performance trade-offs\")\n",
    "print(\"❌ Not validating final model on truly unseen data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd1e9c5",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, you learned:\n",
    "\n",
    "1. **Tuning Basics**: Using `ForecastingGridSearchCV` for parameter optimization\n",
    "2. **Simple Model Tuning**: Optimizing exponential smoothing parameters\n",
    "3. **Pipeline Tuning**: Tuning complex compositions with transformations\n",
    "4. **Model Comparison**: Evaluating tuned vs. baseline models\n",
    "5. **Cross-validation**: Robust evaluation of tuned models\n",
    "6. **Advanced Strategies**: Randomized search and Bayesian optimization concepts\n",
    "7. **Best Practices**: Guidelines for effective hyperparameter tuning\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "- **Separate Validation**: Never tune on your final test set\n",
    "- **Computational Efficiency**: Balance thoroughness with computational cost\n",
    "- **Parameter Analysis**: Understand which parameters matter most\n",
    "- **Robust Evaluation**: Use cross-validation for reliable performance estimates\n",
    "- **Practical Impact**: Consider whether tuning improvements are meaningful\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Explore \"Probabilistic Forecasting\" to extend tuning to uncertainty quantification\n",
    "- Learn \"Global Forecasting\" for advanced model architectures\n",
    "- Try \"Ensemble Methods\" to combine multiple tuned models"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
