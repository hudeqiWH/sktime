{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcd82e25",
   "metadata": {},
   "source": [
    "# Cross-validation and Metrics in Time Series Forecasting\n",
    "\n",
    "This tutorial demonstrates proper cross-validation techniques and evaluation metrics for time series forecasting.\n",
    "\n",
    "**Duration:** ~10 minutes\n",
    "\n",
    "## Learning objectives\n",
    "\n",
    "By the end of this tutorial, you will be able to:\n",
    "- Use time series splitters for proper cross-validation\n",
    "- Visualize cross-validation windows with `plot_windows`\n",
    "- Apply various forecasting metrics\n",
    "- Use the `evaluate` function for comprehensive model assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a2947b",
   "metadata": {},
   "source": [
    "## 1. Time Series Cross-validation Splitters\n",
    "\n",
    "Time series data requires special consideration for cross-validation due to temporal dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a21839c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sktime.datasets import load_airline\n",
    "from sktime.forecasting.exp_smoothing import ExponentialSmoothing\n",
    "from sktime.forecasting.model_evaluation import evaluate\n",
    "from sktime.forecasting.naive import NaiveForecaster\n",
    "from sktime.split import (\n",
    "    ExpandingWindowSplitter,\n",
    "    SlidingWindowSplitter,\n",
    "    temporal_train_test_split,\n",
    ")\n",
    "from sktime.utils.plotting import plot_windows\n",
    "\n",
    "# Load and examine data\n",
    "y = load_airline()\n",
    "print(f\"Dataset: {y.shape[0]} observations from {y.index[0]} to {y.index[-1]}\")\n",
    "\n",
    "# Create different types of splitters\n",
    "splitters = {\n",
    "    \"Expanding Window\": ExpandingWindowSplitter(\n",
    "        initial_window=36,  # Initial training window\n",
    "        step_length=12,  # Step between CV folds\n",
    "        fh=[1, 2, 3, 6, 12],  # Forecast horizons to evaluate\n",
    "    ),\n",
    "    \"Sliding Window\": SlidingWindowSplitter(\n",
    "        window_length=48,  # Fixed training window size\n",
    "        step_length=6,  # Step between CV folds\n",
    "        fh=[1, 3, 6, 12],  # Forecast horizons\n",
    "    ),\n",
    "}\n",
    "\n",
    "for name, splitter in splitters.items():\n",
    "    n_splits = splitter.get_n_splits(y)\n",
    "    print(f\"{name}: {n_splits} splits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1f1f5d",
   "metadata": {},
   "source": [
    "## 2. Visualizing Cross-validation Windows\n",
    "\n",
    "Understanding how your data is split is crucial for proper evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3eecd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize different splitting strategies\n",
    "fig, axes = plt.subplots(2, 1, figsize=(15, 8))\n",
    "\n",
    "# Expanding window\n",
    "plot_windows(\n",
    "    splitters[\"Expanding Window\"],\n",
    "    y,\n",
    "    ax=axes[0],\n",
    "    title=\"Expanding Window Cross-validation\",\n",
    ")\n",
    "\n",
    "# Sliding window\n",
    "plot_windows(\n",
    "    splitters[\"Sliding Window\"], y, ax=axes[1], title=\"Sliding Window Cross-validation\"\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Window Visualization Legend:\")\n",
    "print(\"- Blue: Training data\")\n",
    "print(\"- Orange: Test data (forecast horizon)\")\n",
    "print(\"- Each row represents one cross-validation fold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128f8676",
   "metadata": {},
   "source": [
    "## 3. Forecasting Metrics\n",
    "\n",
    "Different metrics capture different aspects of forecast quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c877e47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.performance_metrics.forecasting import (\n",
    "    geometric_mean_absolute_error,\n",
    "    mean_absolute_error,\n",
    "    mean_absolute_percentage_error,\n",
    "    mean_squared_error,\n",
    "    mean_squared_percentage_error,\n",
    "    median_absolute_error,\n",
    ")\n",
    "\n",
    "# Create sample predictions for demonstration\n",
    "y_train, y_test = temporal_train_test_split(y, test_size=12)\n",
    "\n",
    "# Fit simple forecasters\n",
    "naive_forecaster = NaiveForecaster(strategy=\"seasonal_last\", sp=12)\n",
    "naive_forecaster.fit(y_train)\n",
    "y_pred_naive = naive_forecaster.predict(fh=range(1, 13))\n",
    "\n",
    "exp_smoothing = ExponentialSmoothing(trend=\"add\", seasonal=\"multiplicative\", sp=12)\n",
    "exp_smoothing.fit(y_train)\n",
    "y_pred_exp = exp_smoothing.predict(fh=range(1, 13))\n",
    "\n",
    "# Calculate various metrics\n",
    "metrics = {\n",
    "    \"MAE\": mean_absolute_error,\n",
    "    \"MSE\": mean_squared_error,\n",
    "    \"MAPE\": mean_absolute_percentage_error,\n",
    "    \"MedAE\": median_absolute_error,\n",
    "    \"MSPE\": mean_squared_percentage_error,\n",
    "    \"GMAE\": geometric_mean_absolute_error,\n",
    "}\n",
    "\n",
    "print(\"Forecasting Metrics Comparison:\")\n",
    "print(\"\\nNaive Forecaster:\")\n",
    "for name, metric_func in metrics.items():\n",
    "    try:\n",
    "        value = metric_func(y_test, y_pred_naive)\n",
    "        if name in [\"MAPE\", \"MSPE\"]:\n",
    "            print(f\"{name:>6}: {value:>8.2%}\")\n",
    "        else:\n",
    "            print(f\"{name:>6}: {value:>8.2f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"{name:>6}: Error - {str(e)[:30]}\")\n",
    "\n",
    "print(\"\\nExponential Smoothing:\")\n",
    "for name, metric_func in metrics.items():\n",
    "    try:\n",
    "        value = metric_func(y_test, y_pred_exp)\n",
    "        if name in [\"MAPE\", \"MSPE\"]:\n",
    "            print(f\"{name:>6}: {value:>8.2%}\")\n",
    "        else:\n",
    "            print(f\"{name:>6}: {value:>8.2f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"{name:>6}: Error - {str(e)[:30]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae661c2",
   "metadata": {},
   "source": [
    "## 4. Understanding Different Metrics\n",
    "\n",
    "Each metric has different properties and use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ad504a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Metric Properties and Use Cases:\")\n",
    "\n",
    "print(\"\\n1. MEAN ABSOLUTE ERROR (MAE):\")\n",
    "print(\"   - Units: Same as original data\")\n",
    "print(\"   - Robust to outliers\")\n",
    "print(\"   - Easy to interpret\")\n",
    "print(\"   - Use when: You want interpretable, robust error measure\")\n",
    "\n",
    "print(\"\\n2. MEAN SQUARED ERROR (MSE):\")\n",
    "print(\"   - Units: Squared original units\")\n",
    "print(\"   - Penalizes large errors more\")\n",
    "print(\"   - Sensitive to outliers\")\n",
    "print(\"   - Use when: Large errors are particularly costly\")\n",
    "\n",
    "print(\"\\n3. MEAN ABSOLUTE PERCENTAGE ERROR (MAPE):\")\n",
    "print(\"   - Units: Percentage\")\n",
    "print(\"   - Scale-independent\")\n",
    "print(\"   - Issues with values near zero\")\n",
    "print(\"   - Use when: Comparing across different scales\")\n",
    "\n",
    "print(\"\\n4. MEDIAN ABSOLUTE ERROR (MedAE):\")\n",
    "print(\"   - Units: Same as original data\")\n",
    "print(\"   - Very robust to outliers\")\n",
    "print(\"   - Less sensitive to extreme errors\")\n",
    "print(\"   - Use when: Data has many outliers\")\n",
    "\n",
    "# Demonstrate metric behavior with artificial examples\n",
    "print(\"\\n\\nMetric Behavior Example:\")\n",
    "\n",
    "# Create examples with different error patterns\n",
    "y_true = pd.Series([100, 100, 100, 100, 100])\n",
    "y_pred_consistent = pd.Series([95, 95, 95, 95, 95])  # Consistent small errors\n",
    "y_pred_outlier = pd.Series([100, 100, 100, 100, 50])  # One large error\n",
    "\n",
    "examples = {\n",
    "    \"Consistent Small Errors\": y_pred_consistent,\n",
    "    \"One Large Error\": y_pred_outlier,\n",
    "}\n",
    "\n",
    "for scenario, y_pred in examples.items():\n",
    "    print(f\"\\n{scenario}:\")\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    medae = median_absolute_error(y_true, y_pred)\n",
    "\n",
    "    print(f\"  MAE:  {mae:.1f}\")\n",
    "    print(f\"  RMSE: {rmse:.1f}\")\n",
    "    print(f\"  MedAE: {medae:.1f}\")\n",
    "    print(f\"  Errors: {(y_true - y_pred).abs().tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4cec6c",
   "metadata": {},
   "source": [
    "## 5. Comprehensive Evaluation with evaluate()\n",
    "\n",
    "The `evaluate` function provides a comprehensive assessment across multiple CV folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc4f93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up comprehensive evaluation\n",
    "forecasters = {\n",
    "    \"Naive\": NaiveForecaster(strategy=\"seasonal_last\", sp=12),\n",
    "    \"ExponentialSmoothing\": ExponentialSmoothing(\n",
    "        trend=\"add\", seasonal=\"multiplicative\", sp=12\n",
    "    ),\n",
    "}\n",
    "\n",
    "# Use expanding window splitter for evaluation\n",
    "cv_splitter = ExpandingWindowSplitter(\n",
    "    initial_window=36, step_length=6, fh=[1, 3, 6, 12]\n",
    ")\n",
    "\n",
    "# Define metrics to evaluate\n",
    "scoring = [\n",
    "    \"mean_absolute_error\",\n",
    "    \"mean_squared_error\",\n",
    "    \"mean_absolute_percentage_error\",\n",
    "]\n",
    "\n",
    "print(\"Running comprehensive evaluation...\")\n",
    "print(f\"CV Splitter: {cv_splitter.get_n_splits(y)} folds\")\n",
    "print(f\"Forecast horizons: {cv_splitter.fh}\")\n",
    "print(f\"Metrics: {scoring}\")\n",
    "\n",
    "# Evaluate forecasters\n",
    "results = {}\n",
    "for name, forecaster in forecasters.items():\n",
    "    print(f\"\\nEvaluating {name}...\")\n",
    "    result = evaluate(\n",
    "        forecaster=forecaster, y=y, cv=cv_splitter, scoring=scoring, return_data=True\n",
    "    )\n",
    "    results[name] = result\n",
    "    print(f\"Completed {name}\")\n",
    "\n",
    "print(\"\\nEvaluation completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1cd0f5",
   "metadata": {},
   "source": [
    "## 6. Analyzing Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b39eaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze results\n",
    "print(\"Cross-validation Results Summary:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for name, result in results.items():\n",
    "    print(f\"\\n{name.upper()}:\")\n",
    "\n",
    "    # Get the evaluation metrics\n",
    "    if hasattr(result, \"columns\"):  # DataFrame result\n",
    "        metrics_df = result\n",
    "    else:  # Dictionary result\n",
    "        metrics_df = pd.DataFrame(result)\n",
    "\n",
    "    print(metrics_df.describe())\n",
    "\n",
    "# Compare forecasters\n",
    "print(\"\\n\\nFORECASTER COMPARISON:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "comparison_metrics = []\n",
    "for name, result in results.items():\n",
    "    if hasattr(result, \"columns\"):  # DataFrame result\n",
    "        metrics_df = result\n",
    "    else:\n",
    "        metrics_df = pd.DataFrame(result)\n",
    "\n",
    "    # Calculate mean performance across folds\n",
    "    mean_metrics = metrics_df.mean()\n",
    "    mean_metrics.name = name\n",
    "    comparison_metrics.append(mean_metrics)\n",
    "\n",
    "if comparison_metrics:\n",
    "    comparison_df = pd.concat(comparison_metrics, axis=1)\n",
    "    print(comparison_df)\n",
    "\n",
    "    # Find best forecaster for each metric\n",
    "    print(\"\\nBest Forecaster by Metric:\")\n",
    "    for metric in comparison_df.index:\n",
    "        if \"error\" in metric.lower():\n",
    "            best = comparison_df.loc[metric].idxmin()  # Lower is better\n",
    "        else:\n",
    "            best = comparison_df.loc[metric].idxmax()  # Higher is better\n",
    "        print(f\"{metric}: {best}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2a2d75",
   "metadata": {},
   "source": [
    "## 7. Forecast Horizon Analysis\n",
    "\n",
    "Understanding how forecast quality changes with horizon length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f149e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze performance by forecast horizon\n",
    "print(\"Performance by Forecast Horizon:\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Create detailed horizon analysis\n",
    "horizon_analysis = {}\n",
    "\n",
    "for name, result in results.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "\n",
    "    if hasattr(result, \"columns\"):  # DataFrame result\n",
    "        metrics_df = result\n",
    "    else:\n",
    "        metrics_df = pd.DataFrame(result)\n",
    "\n",
    "    # Group by forecast horizon if available\n",
    "    if \"fh\" in metrics_df.columns:\n",
    "        horizon_stats = metrics_df.groupby(\"fh\").agg([\"mean\", \"std\"])\n",
    "        print(horizon_stats)\n",
    "        horizon_analysis[name] = horizon_stats\n",
    "    else:\n",
    "        # If fh not in columns, show overall statistics\n",
    "        print(\"Forecast horizon information not available in results\")\n",
    "        print(metrics_df.describe())\n",
    "\n",
    "# Visualize horizon performance if available\n",
    "if horizon_analysis:\n",
    "    print(\"\\nCreating horizon performance visualization...\")\n",
    "\n",
    "    # Plot performance by horizon for MAPE\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    for name, stats in horizon_analysis.items():\n",
    "        if \"mean_absolute_percentage_error\" in stats.columns:\n",
    "            mape_mean = stats[(\"mean_absolute_percentage_error\", \"mean\")]\n",
    "            mape_std = stats[(\"mean_absolute_percentage_error\", \"std\")]\n",
    "\n",
    "            ax.plot(mape_mean.index, mape_mean.values, \"o-\", label=name)\n",
    "            ax.fill_between(\n",
    "                mape_mean.index,\n",
    "                mape_mean.values - mape_std.values,\n",
    "                mape_mean.values + mape_std.values,\n",
    "                alpha=0.3,\n",
    "            )\n",
    "\n",
    "    ax.set_xlabel(\"Forecast Horizon\")\n",
    "    ax.set_ylabel(\"MAPE\")\n",
    "    ax.set_title(\"Forecast Performance by Horizon\")\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"\\nDetailed horizon analysis not available with current results structure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c76419f",
   "metadata": {},
   "source": [
    "## 8. Best Practices for Cross-validation\n",
    "\n",
    "Key guidelines for proper time series evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d247e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cross-validation Best Practices:\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "print(\"\\n1. TEMPORAL ORDERING:\")\n",
    "print(\"   ✓ Always respect temporal order\")\n",
    "print(\"   ✓ Training data must come before test data\")\n",
    "print(\"   ✓ Never use future information for training\")\n",
    "\n",
    "print(\"\\n2. SPLITTER SELECTION:\")\n",
    "print(\"   • ExpandingWindowSplitter: When you want to use all available history\")\n",
    "print(\"   • SlidingWindowSplitter: When recent data is most relevant\")\n",
    "print(\"   • Consider data size and computational constraints\")\n",
    "\n",
    "print(\"\\n3. FORECAST HORIZON:\")\n",
    "print(\"   • Test multiple horizons: [1, 3, 6, 12]\")\n",
    "print(\"   • Include your actual use case horizon\")\n",
    "print(\"   • Consider seasonal patterns (e.g., 12 for monthly data)\")\n",
    "\n",
    "print(\"\\n4. METRICS SELECTION:\")\n",
    "print(\"   • Use multiple metrics to get complete picture\")\n",
    "print(\"   • MAPE for scale-independent comparison\")\n",
    "print(\"   • MAE/RMSE for absolute error understanding\")\n",
    "print(\"   • Choose metrics aligned with business objectives\")\n",
    "\n",
    "print(\"\\n5. STATISTICAL SIGNIFICANCE:\")\n",
    "print(\"   • Use sufficient CV folds (typically 3-10)\")\n",
    "print(\"   • Report confidence intervals when possible\")\n",
    "print(\"   • Consider seasonal effects in fold selection\")\n",
    "\n",
    "# Demonstrate proper vs improper CV\n",
    "print(\"\\n\\nCOMMON MISTAKES TO AVOID:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "print(\"\\n❌ WRONG - Random splits:\")\n",
    "print(\"   from sklearn.model_selection import KFold\")\n",
    "print(\"   cv = KFold(n_splits=5, shuffle=True)  # Destroys temporal order!\")\n",
    "\n",
    "print(\"\\n✓ CORRECT - Temporal splits:\")\n",
    "print(\"   from sktime.split import ExpandingWindowSplitter\")\n",
    "print(\"   cv = ExpandingWindowSplitter(initial_window=36)\")\n",
    "\n",
    "print(\"\\n❌ WRONG - Using future data for preprocessing:\")\n",
    "print(\"   scaler.fit(X_full)  # Uses future data!\")\n",
    "print(\"   X_scaled = scaler.transform(X_full)\")\n",
    "\n",
    "print(\"\\n✓ CORRECT - Preprocessing in pipeline:\")\n",
    "print(\"   from sktime.forecasting.compose import ForecastingPipeline\")\n",
    "print(\"   pipeline = ForecastingPipeline([('scale', scaler), ('forecast', model)])\")\n",
    "\n",
    "print(\"\\n❌ WRONG - Single metric evaluation:\")\n",
    "print(\"   # Only using RMSE might miss important patterns\")\n",
    "\n",
    "print(\"\\n✓ CORRECT - Multiple metrics:\")\n",
    "print(\"   scoring = ['mean_absolute_error', 'mean_absolute_percentage_error', ...]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559dad1d",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, you learned:\n",
    "\n",
    "1. **Time Series Splitters**: `ExpandingWindowSplitter` and `SlidingWindowSplitter` for proper CV\n",
    "2. **Window Visualization**: Using `plot_windows` to understand data splits\n",
    "3. **Forecasting Metrics**: MAE, MSE, MAPE, MedAE and their properties\n",
    "4. **Comprehensive Evaluation**: Using `evaluate()` for robust model assessment\n",
    "5. **Results Analysis**: Interpreting CV results and comparing forecasters\n",
    "6. **Horizon Analysis**: Understanding how performance varies with forecast distance\n",
    "7. **Best Practices**: Guidelines for proper time series evaluation\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "- **Temporal Order**: Always respect time ordering in cross-validation\n",
    "- **Multiple Metrics**: Use several metrics to get a complete picture\n",
    "- **Multiple Horizons**: Test performance at different forecast distances\n",
    "- **Statistical Rigor**: Use sufficient folds and report uncertainty\n",
    "- **Business Alignment**: Choose metrics that reflect real-world costs\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Learn \"Hyperparameter Tuning\" to optimize model performance using CV\n",
    "- Explore \"Probabilistic Forecasting\" for uncertainty quantification\n",
    "- Try \"Global Forecasting\" for advanced evaluation techniques"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
